{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import math\n",
    "\n",
    "\n",
    "class TFIDFVectorizer:\n",
    "    def __init__(self, es_client: Elasticsearch, index_name: str, analyzer_name: str):\n",
    "        self.client = es_client\n",
    "        self.index_name = index_name\n",
    "        self.analyzer_name = analyzer_name\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        given query string it outputs the list of preprocessed tokens from it\n",
    "        using same analyzer (preprocessing pipeline) than the arxiv abstracts\n",
    "        \"\"\"\n",
    "\n",
    "        # Use the analyze API on the specified index\n",
    "        response = self.client.indices.analyze(\n",
    "            index=self.index_name, body={\"analyzer\": self.analyzer_name, \"text\": text}\n",
    "        )\n",
    "\n",
    "        # Extract just the token strings from the response\n",
    "        preprocessed_terms = [token_info[\"token\"] for token_info in response[\"tokens\"]]\n",
    "        return preprocessed_terms\n",
    "\n",
    "    def doc_count(self) -> int:\n",
    "        \"\"\"Returns the number of documents in the index.\"\"\"\n",
    "        response = self.client.count(index=self.index_name)\n",
    "        return response[\"count\"]\n",
    "\n",
    "    def _doc_terms_info(self, doc_id: str) -> Dict[str, Dict[str, int]]:\n",
    "        \"\"\"Retrieves term statistics for the specified document.\"\"\"\n",
    "        response = self.client.termvectors(\n",
    "            index=self.index_name,\n",
    "            id=doc_id,\n",
    "            fields=[\"text\"],\n",
    "            term_statistics=True,\n",
    "            positions=False,\n",
    "            offsets=False,\n",
    "            payloads=False,\n",
    "        )\n",
    "        terms_info = response.get(\"term_vectors\", {}).get(\"text\", {}).get(\"terms\", {})\n",
    "        return terms_info\n",
    "\n",
    "    def _text_terms_info(self, text: str) -> Dict[str, Dict[str, int]]:\n",
    "        \"\"\"Retrieves term statistics for the given text.\"\"\"\n",
    "        response = self.client.termvectors(\n",
    "            index=self.index_name,\n",
    "            doc={\"text\": text},\n",
    "            fields=[\"text\"],\n",
    "            term_statistics=True,\n",
    "            positions=False,\n",
    "            offsets=False,\n",
    "            payloads=False,\n",
    "        )\n",
    "        terms_info = response.get(\"term_vectors\", {}).get(\"text\", {}).get(\"terms\", {})\n",
    "        return terms_info\n",
    "\n",
    "    def tf_idf_document(self, doc_id: str) -> Dict[str, float]:\n",
    "        \"\"\"Computes the TF-IDF weights for terms in the specified document.\"\"\"\n",
    "\n",
    "        tf_idf_weights = {}\n",
    "        doc_count = self.doc_count()\n",
    "        terms_info = self._doc_terms_info(doc_id)\n",
    "        for term, stats in terms_info.items():\n",
    "            tf = stats.get(\"term_freq\", 0)\n",
    "            df = stats.get(\"doc_freq\", 1)  # avoid division by zero\n",
    "            idf = math.log2(doc_count / df)\n",
    "            tf_idf_weights[term] = tf * idf\n",
    "\n",
    "        return tf_idf_weights\n",
    "\n",
    "    def tf_idf_text(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Computes the TF-IDF weights for terms in the text.\"\"\"\n",
    "        tf_idf_weights = {}\n",
    "        doc_count = self.doc_count()\n",
    "        terms_info = self._text_terms_info(text)\n",
    "        for term, stats in terms_info.items():\n",
    "            tf = stats.get(\"term_freq\", 0)\n",
    "            df = stats.get(\"doc_freq\", 1)  # avoid division by zero\n",
    "            idf = math.log2(doc_count / df)\n",
    "            tf_idf_weights[term] = tf * idf\n",
    "\n",
    "        return tf_idf_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning': 3.1174318645927173, 'machine': 4.130636908016368}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TFIDFVectorizer(client, index_name=\"arxiv\", analyzer_name=\"custom\")\n",
    "vectorizer.tf_idf_text(\"machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"machine learning\"\n",
    "index_name = \"arxiv\"\n",
    "analyzer_name = \"custom\"\n",
    "r = 10  # Number of results to return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing search of query string 'machine learning' with over documents in index 'arxiv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58102/58102 [45:32<00:00, 21.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('30749', 0.499339916162954),\n",
      " ('29106', 0.4550250906242585),\n",
      " ('25230', 0.45238527762709785),\n",
      " ('30639', 0.4255856152643423),\n",
      " ('18989', 0.3944888658727178),\n",
      " ('53877', 0.3944888658727178),\n",
      " ('37272', 0.3767314656563681),\n",
      " ('55842', 0.3767314656563681),\n",
      " ('26955', 0.35511371168748196),\n",
      " ('35642', 0.3518888496131795)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"Executing search of query string '{query_string}' with over documents in index '{index_name}'\")\n",
    "vectorizer = TFIDFVectorizer(client, index_name=index_name, analyzer_name=analyzer_name)\n",
    "\n",
    "# Compute the query vector. No need to normalize it, as we only care about relative similarities.\n",
    "query_vector = vectorizer.tf_idf_text(query_string)\n",
    "query_norm = math.sqrt(sum(weight**2 for weight in query_vector.values()))\n",
    "query_vector_normalized = {token: weight / query_norm for token, weight in query_vector.items()}\n",
    "\n",
    "# Naive implementation:\n",
    "# Scan through docs, compute cosine sim between query and each doc\n",
    "similarities = {}\n",
    "for doc in tqdm(scan(client, index=index_name, query={\"_source\": False}), total=vectorizer.doc_count()):\n",
    "    docid = doc[\"_id\"]\n",
    "    doc_tf_idf = vectorizer.tf_idf_document(docid)\n",
    "    doc_tf_idf_norm = math.sqrt(sum(weight**2 for weight in doc_tf_idf.values()))\n",
    "    similarities[docid] = 0.0\n",
    "    for token, weight in query_vector_normalized.items():\n",
    "        similarities[docid] += weight * doc_tf_idf.get(token, 0)\n",
    "    similarities[docid] /= doc_tf_idf_norm\n",
    "\n",
    "# now sort by cosine similarity\n",
    "sorted_answer = sorted(similarities.items(), key=lambda kv: kv[1], reverse=True)\n",
    "pprint(sorted_answer[:r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Term-First Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to first create the inverted index for the documents. We'll store, for each term, a list of documents that contain\n",
    "that term, along with the TF-IDF weight of that term in the document.\n",
    "\n",
    "We'll ignore any compression optimizations for simplicity. This won't cause any issues, as it will still fit in memory.\n",
    "\n",
    "Also, one of the interesting things of the inverted-index approach is that we only need to compute it once, and then we can store it on disk\n",
    "for future queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58102/58102 [45:51<00:00, 21.11it/s] \n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "vectorizer = TFIDFVectorizer(client, index_name=index_name, analyzer_name=analyzer_name)\n",
    "inverted_index: Dict[str, List[Tuple[int, float]]] = defaultdict(list)  # Lists are amortized O(1) for appends\n",
    "doc_ids: List[str] = []  # Storing strings in the inverted index would be too memory intensive\n",
    "\n",
    "\n",
    "# We don't care about the order of the docIDs (in boolean models we would care, to implement fast merging algorithms)\n",
    "for doc_index, doc in tqdm(\n",
    "    enumerate(scan(client, index=index_name, query={\"_source\": False})), total=vectorizer.doc_count()\n",
    "):\n",
    "    docid = doc[\"_id\"]\n",
    "    doc_ids.append(docid)\n",
    "    doc_tf_idf = vectorizer.tf_idf_document(docid)\n",
    "    for term, weight in doc_tf_idf.items():\n",
    "        inverted_index[term].append((doc_index, weight))\n",
    "        # For typing consistency, we'll store tuples, instead of having a single \"sausage\" list of weights\n",
    "inverted_index = dict(inverted_index)  # Convert back to normal dict for pickling\n",
    "\n",
    "os.makedirs(\"../data/cache/\", exist_ok=True)\n",
    "\n",
    "# Save the inverted index to disk\n",
    "with open(\"../data/cache/inverted_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(inverted_index, f)\n",
    "\n",
    "# Save the document IDs to disk\n",
    "with open(\"../data/cache/doc_ids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(doc_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without caching of TF-IDF norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing quick search of query string 'machine learning' with over documents in index 'arxiv'\n",
      "Vectorizer loaded in 0.00 seconds\n",
      "Query vector computed in 0.05 seconds\n",
      "Inverted index loaded in 1.40 seconds\n",
      "Accumulated similarities for 7392 documents in 0.00 seconds\n",
      "Normalized similarities in 344.75 seconds\n",
      "Sorted top 10 results in 0.20 seconds\n",
      "Total query time: 346.40 seconds\n",
      "[('30749', 0.499339916162954),\n",
      " ('29106', 0.4550250906242585),\n",
      " ('25230', 0.45238527762709785),\n",
      " ('30639', 0.4255856152643423),\n",
      " ('18989', 0.3944888658727178),\n",
      " ('53877', 0.3944888658727178),\n",
      " ('37272', 0.3767314656563681),\n",
      " ('55842', 0.3767314656563681),\n",
      " ('26955', 0.35511371168748196),\n",
      " ('35642', 0.3518888496131795)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "\n",
    "print(f\"Executing quick search of query string '{query_string}' with over documents in index '{index_name}'\")\n",
    "start = time.time()\n",
    "vectorizer = TFIDFVectorizer(client, index_name=index_name, analyzer_name=analyzer_name)\n",
    "vectorizer_loaded_time = time.time()\n",
    "print(f\"Vectorizer loaded in {vectorizer_loaded_time-start:.2f} seconds\")\n",
    "\n",
    "# Compute the query vector. No need to normalize it, as we only care about relative similarities.\n",
    "query_vector = vectorizer.tf_idf_text(query_string)\n",
    "query_norm = math.sqrt(sum(weight**2 for weight in query_vector.values()))\n",
    "query_vector_normalized = {token: weight / query_norm for token, weight in query_vector.items()}\n",
    "\n",
    "\n",
    "query_vector_computed_time = time.time()\n",
    "print(f\"Query vector computed in {query_vector_computed_time-vectorizer_loaded_time:.2f} seconds\")\n",
    "\n",
    "# Fast implementation:\n",
    "# Scan through the query terms, look them up in the inverted index, and accumulate partial similarities\n",
    "\n",
    "# Start by loading the inverted index and document IDs from disk\n",
    "with open(\"../data/cache/inverted_index.pkl\", \"rb\") as f:\n",
    "    inverted_index = pickle.load(f)\n",
    "with open(\"../data/cache/doc_ids.pkl\", \"rb\") as f:\n",
    "    doc_ids = pickle.load(f)\n",
    "\n",
    "inverted_index_loaded_time = time.time()\n",
    "print(f\"Inverted index loaded in {inverted_index_loaded_time-query_vector_computed_time:.2f} seconds\")\n",
    "\n",
    "similarities = defaultdict(float)\n",
    "for term, weight in query_vector_normalized.items():\n",
    "    if term in inverted_index:\n",
    "        for doc_index, doc_weight in inverted_index[term]:\n",
    "            docid = doc_ids[doc_index]\n",
    "            similarities[docid] += weight * doc_weight\n",
    "\n",
    "similarities_aggregated_time = time.time()\n",
    "print(\n",
    "    f\"Accumulated similarities for {len(similarities)} documents in {similarities_aggregated_time-inverted_index_loaded_time:.2f} seconds\"\n",
    ")\n",
    "\n",
    "for docid in similarities:\n",
    "    doc_tf_idf = vectorizer.tf_idf_document(docid)\n",
    "    doc_tf_idf_norm = math.sqrt(sum(weight**2 for weight in doc_tf_idf.values()))\n",
    "    similarities[docid] /= doc_tf_idf_norm\n",
    "\n",
    "similarities_normalized_time = time.time()\n",
    "print(f\"Normalized similarities in {similarities_normalized_time-similarities_aggregated_time:.2f} seconds\")\n",
    "\n",
    "# now sort by cosine similarity\n",
    "sorted_answer = sorted(similarities.items(), key=lambda kv: kv[1], reverse=True)\n",
    "response_time = time.time()\n",
    "print(f\"Sorted top {r} results in {response_time-similarities_normalized_time:.2f} seconds\")\n",
    "print(f\"Total query time: {response_time-start:.2f} seconds\")\n",
    "pprint(sorted_answer[:r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, querying elasticsearch each time for the norm of the tf-idf vectors of the documents is very inefficient, so we'll try to also cache this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With caching of TF-IDF norms\n",
    "\n",
    "To ease the computation, we'll use async calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import math\n",
    "from elasticsearch import AsyncElasticsearch\n",
    "\n",
    "\n",
    "class AsyncTFIDFVectorizer:\n",
    "    def __init__(self, es_client: AsyncElasticsearch, index_name: str, analyzer_name: str):\n",
    "        self.client = es_client\n",
    "        self.index_name = index_name\n",
    "        self.analyzer_name = analyzer_name\n",
    "\n",
    "    async def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        given query string it outputs the list of preprocessed tokens from it\n",
    "        using same analyzer (preprocessing pipeline) than the arxiv abstracts\n",
    "        \"\"\"\n",
    "\n",
    "        # Use the analyze API on the specified index\n",
    "        response = await self.client.indices.analyze(\n",
    "            index=self.index_name, body={\"analyzer\": self.analyzer_name, \"text\": text}\n",
    "        )\n",
    "\n",
    "        # Extract just the token strings from the response\n",
    "        preprocessed_terms = [token_info[\"token\"] for token_info in response[\"tokens\"]]\n",
    "        return preprocessed_terms\n",
    "\n",
    "    async def doc_count(self) -> int:\n",
    "        \"\"\"Returns the number of documents in the index.\"\"\"\n",
    "        response = await self.client.count(index=self.index_name)\n",
    "        return response[\"count\"]\n",
    "\n",
    "    async def _doc_terms_info(self, doc_id: str) -> Dict[str, Dict[str, int]]:\n",
    "        \"\"\"Retrieves term statistics for the specified document.\"\"\"\n",
    "        response = await self.client.termvectors(\n",
    "            index=self.index_name,\n",
    "            id=doc_id,\n",
    "            fields=[\"text\"],\n",
    "            term_statistics=True,\n",
    "            positions=False,\n",
    "            offsets=False,\n",
    "            payloads=False,\n",
    "        )\n",
    "        terms_info = response.get(\"term_vectors\", {}).get(\"text\", {}).get(\"terms\", {})\n",
    "        return terms_info\n",
    "\n",
    "    async def _text_terms_info(self, text: str) -> Dict[str, Dict[str, int]]:\n",
    "        \"\"\"Retrieves term statistics for the given text.\"\"\"\n",
    "        response = await self.client.termvectors(\n",
    "            index=self.index_name,\n",
    "            doc={\"text\": text},\n",
    "            fields=[\"text\"],\n",
    "            term_statistics=True,\n",
    "            positions=False,\n",
    "            offsets=False,\n",
    "            payloads=False,\n",
    "        )\n",
    "        terms_info = response.get(\"term_vectors\", {}).get(\"text\", {}).get(\"terms\", {})\n",
    "        return terms_info\n",
    "\n",
    "    async def tf_idf_document(self, doc_id: str) -> Dict[str, float]:\n",
    "        \"\"\"Computes the TF-IDF weights for terms in the specified document.\"\"\"\n",
    "\n",
    "        tf_idf_weights = {}\n",
    "        doc_count = await self.doc_count()\n",
    "        terms_info = await self._doc_terms_info(doc_id)\n",
    "        for term, stats in terms_info.items():\n",
    "            tf = stats.get(\"term_freq\", 0)\n",
    "            df = stats.get(\"doc_freq\", 1)  # avoid division by zero\n",
    "            idf = math.log2(doc_count / df)\n",
    "            tf_idf_weights[term] = tf * idf\n",
    "\n",
    "        return tf_idf_weights\n",
    "\n",
    "    async def tf_idf_text(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Computes the TF-IDF weights for terms in the text.\"\"\"\n",
    "        tf_idf_weights = {}\n",
    "        doc_count = await self.doc_count()\n",
    "        terms_info = await self._text_terms_info(text)\n",
    "        for term, stats in terms_info.items():\n",
    "            tf = stats.get(\"term_freq\", 0)\n",
    "            df = stats.get(\"doc_freq\", 1)  # avoid division by zero\n",
    "            idf = math.log2(doc_count / df)\n",
    "            tf_idf_weights[term] = tf * idf\n",
    "\n",
    "        return tf_idf_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58102 [00:16<?, ?it/s]\n",
      "C:\\Users\\mirxm\\AppData\\Local\\Temp\\ipykernel_27844\\654239101.py:24: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  tasks = [compute_norm(docid) for docid in doc_ids]\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "100%|██████████| 58102/58102 [00:26<00:00, 2172.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "async_client = AsyncElasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "vectorizer = AsyncTFIDFVectorizer(async_client, index_name=index_name, analyzer_name=analyzer_name)\n",
    "vector_norms: List[float] = []  # Again, we'll store floats in a list for memory and access efficiency\n",
    "\n",
    "\n",
    "with open(\"../data/cache/doc_ids.pkl\", \"rb\") as f:\n",
    "    doc_ids: List[str] = pickle.load(f)\n",
    "\n",
    "bar = tqdm(total=len(doc_ids))\n",
    "\n",
    "\n",
    "async def compute_norm(docid: str) -> float:\n",
    "    doc_tf_idf = await vectorizer.tf_idf_document(docid)\n",
    "    doc_tf_idf_norm = math.sqrt(sum(weight**2 for weight in doc_tf_idf.values()))\n",
    "    bar.update(1)\n",
    "    return doc_tf_idf_norm\n",
    "\n",
    "\n",
    "async def main():\n",
    "    tasks = [compute_norm(docid) for docid in doc_ids]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "vector_norms = await main()\n",
    "bar.close()\n",
    "\n",
    "with open(\"../data/cache/vector_norms.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vector_norms, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small pat on my own back: This process would've taken 45 minutes if it weren't for the async calls. With them, it took 26 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing quick search of query string 'machine learning' with over documents in index 'arxiv'\n",
      "Vectorizer loaded in 0.00 seconds\n",
      "Query vector computed in 0.04 seconds\n",
      "Inverted index loaded in 0.97 seconds\n",
      "Accumulated similarities for 7392 documents in 0.01 seconds\n",
      "Normalized similarities in 0.00 seconds\n",
      "Sorted top 10 results in 0.00 seconds\n",
      "Total query time: 1.02 seconds\n",
      "(30749, 0.499339916162954)\n",
      "(29106, 0.4550250906242585)\n",
      "(25230, 0.45238527762709785)\n",
      "(30639, 0.4255856152643423)\n",
      "(18989, 0.3944888658727178)\n",
      "(53877, 0.3944888658727178)\n",
      "(37272, 0.3767314656563681)\n",
      "(55842, 0.3767314656563681)\n",
      "(26955, 0.35511371168748196)\n",
      "(35642, 0.3518888496131795)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "print(f\"Executing quick search of query string '{query_string}' with over documents in index '{index_name}'\")\n",
    "start = time.time()\n",
    "vectorizer = TFIDFVectorizer(client, index_name=index_name, analyzer_name=analyzer_name)\n",
    "vectorizer_loaded_time = time.time()\n",
    "print(f\"Vectorizer loaded in {vectorizer_loaded_time-start:.2f} seconds\")\n",
    "\n",
    "# Compute the query vector. No need to normalize it, as we only care about relative similarities.\n",
    "query_vector = vectorizer.tf_idf_text(query_string)\n",
    "query_norm = math.sqrt(sum(weight**2 for weight in query_vector.values()))\n",
    "query_vector_normalized = {token: weight / query_norm for token, weight in query_vector.items()}\n",
    "\n",
    "\n",
    "query_vector_computed_time = time.time()\n",
    "print(f\"Query vector computed in {query_vector_computed_time-vectorizer_loaded_time:.2f} seconds\")\n",
    "\n",
    "# Fast implementation:\n",
    "# Scan through the query terms, look them up in the inverted index, and accumulate partial similarities\n",
    "\n",
    "# Start by loading the inverted index and document IDs from disk\n",
    "with open(\"../data/cache/inverted_index.pkl\", \"rb\") as f:\n",
    "    inverted_index = pickle.load(f)\n",
    "with open(\"../data/cache/doc_ids.pkl\", \"rb\") as f:\n",
    "    doc_ids = pickle.load(f)\n",
    "with open(\"../data/cache/vector_norms.pkl\", \"rb\") as f:\n",
    "    vector_norms = pickle.load(f)\n",
    "\n",
    "inverted_index_loaded_time = time.time()\n",
    "print(f\"Inverted index loaded in {inverted_index_loaded_time-query_vector_computed_time:.2f} seconds\")\n",
    "\n",
    "similarities = defaultdict(float)\n",
    "for term, weight in query_vector_normalized.items():\n",
    "    if term in inverted_index:\n",
    "        for doc_index, doc_weight in inverted_index[term]:\n",
    "            similarities[doc_index] += weight * doc_weight\n",
    "\n",
    "similarities_aggregated_time = time.time()\n",
    "print(\n",
    "    f\"Accumulated similarities for {len(similarities)} documents in {similarities_aggregated_time-inverted_index_loaded_time:.2f} seconds\"\n",
    ")\n",
    "\n",
    "for doc_index in similarities:\n",
    "    similarities[doc_index] /= vector_norms[doc_index]\n",
    "\n",
    "similarities_normalized_time = time.time()\n",
    "print(f\"Normalized similarities in {similarities_normalized_time-similarities_aggregated_time:.2f} seconds\")\n",
    "\n",
    "# now sort by cosine similarity\n",
    "sorted_answer = sorted(similarities.items(), key=lambda kv: kv[1], reverse=True)\n",
    "response_time = time.time()\n",
    "print(f\"Sorted top {r} results in {response_time-similarities_normalized_time:.2f} seconds\")\n",
    "print(f\"Total query time: {response_time-start:.2f} seconds\")\n",
    "for doc_index, score in sorted_answer[:r]:\n",
    "    docid = doc_ids[doc_index]\n",
    "    print(f\"({docid}, {score})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila, what took 45 minutes the first time, was reduced to 1.02 seconds!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab4 (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
