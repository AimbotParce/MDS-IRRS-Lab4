{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.curdir, \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [`elastic/vectorizer.py`](../elastic/vectorizer.py) for the implementation of the `TFIDFVectorizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning': 3.1174318645927173, 'machine': 4.130636908016368}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elastic.vectorizer import TFIDFVectorizer\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# A small test\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "vectorizer = TFIDFVectorizer(client, index_name=\"arxiv\", analyzer_name=\"custom\")\n",
    "vectorizer.tf_idf_text(\"machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"machine learning\"\n",
    "index_name = \"arxiv\"\n",
    "analyzer_name = \"custom\"\n",
    "r = 10  # Number of results to return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing search of query string 'machine learning' with over documents in index 'arxiv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58102/58102 [42:42<00:00, 22.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('30749', 0.499339916162954),\n",
      " ('29106', 0.4550250906242585),\n",
      " ('25230', 0.45238527762709785),\n",
      " ('30639', 0.4255856152643423),\n",
      " ('18989', 0.3944888658727178),\n",
      " ('53877', 0.3944888658727178),\n",
      " ('37272', 0.3767314656563681),\n",
      " ('55842', 0.3767314656563681),\n",
      " ('26955', 0.35511371168748196),\n",
      " ('35642', 0.3518888496131795)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "print(f\"Executing search of query string '{query_string}' with over documents in index '{index_name}'\")\n",
    "vectorizer = TFIDFVectorizer(client, index_name=index_name, analyzer_name=analyzer_name)\n",
    "\n",
    "# Compute the query vector. No need to normalize it, as we only care about relative similarities.\n",
    "query_vector = vectorizer.tf_idf_text(query_string)\n",
    "query_norm = math.sqrt(sum(weight**2 for weight in query_vector.values()))\n",
    "query_vector_normalized = {token: weight / query_norm for token, weight in query_vector.items()}\n",
    "\n",
    "# Naive implementation:\n",
    "# Scan through docs, compute cosine sim between query and each doc\n",
    "similarities = {}\n",
    "for doc in tqdm(scan(client, index=index_name, query={\"_source\": False}), total=vectorizer.doc_count()):\n",
    "    docid = doc[\"_id\"]\n",
    "    doc_tf_idf = vectorizer.tf_idf_document(docid)\n",
    "    doc_tf_idf_norm = math.sqrt(sum(weight**2 for weight in doc_tf_idf.values()))\n",
    "    similarities[docid] = 0.0\n",
    "    for token, weight in query_vector_normalized.items():\n",
    "        similarities[docid] += weight * doc_tf_idf.get(token, 0)\n",
    "    similarities[docid] /= doc_tf_idf_norm\n",
    "\n",
    "# now sort by cosine similarity\n",
    "sorted_answer = sorted(similarities.items(), key=lambda kv: kv[1], reverse=True)\n",
    "pprint(sorted_answer[:r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Term-First Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to first create the inverted index for the documents. We'll store, for each term, a list of documents that contain\n",
    "that term, along with the TF-IDF weight of that term in the document.\n",
    "\n",
    "We'll ignore any compression optimizations for simplicity. This won't cause any issues, as it will still fit in memory.\n",
    "\n",
    "Also, one of the interesting things of the inverted-index approach is that we only need to compute it once, and then we can store it on disk\n",
    "for future queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease the computation, we'll use async calls. See [`elastic/vectorizer.py`](../elastic/vectorizer.py) for the implementation of the `AsyncTFIDFVectorizer` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without caching of TF-IDF norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be the typical implementation. However, because we need the norms of the document vectors, and to get them, we need to query elasticsearch, this becomes very inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58102/58102 [00:29<00:00, 1964.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from collections import defaultdict\n",
    "from elasticsearch.helpers import scan\n",
    "import pickle\n",
    "import os\n",
    "import asyncio\n",
    "from typing import List, Dict\n",
    "from elasticsearch import AsyncElasticsearch\n",
    "from elastic.vectorizer import AsyncTFIDFVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "async_client = AsyncElasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "vectorizer = AsyncTFIDFVectorizer(async_client, index_name=index_name, analyzer_name=analyzer_name)\n",
    "inverted_index: Dict[str, List[Tuple[int, float]]] = defaultdict(list)  # Lists are amortized O(1) for appends\n",
    "doc_ids: List[str]  # Storing strings in the inverted index would be too memory intensive\n",
    "\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)  # We need a sync client for the scan helper\n",
    "doc_ids = [doc[\"_id\"] for doc in scan(client, index=index_name, query={\"_source\": False})]\n",
    "\n",
    "\n",
    "async def process_document(docid: str, doc_index: int):\n",
    "    doc_tf_idf = await vectorizer.tf_idf_document(docid)\n",
    "    for term, weight in doc_tf_idf.items():\n",
    "        inverted_index[term].append((doc_index, weight))\n",
    "    bar.update(1)\n",
    "\n",
    "\n",
    "# We don't care about the order of the docIDs (in boolean models we would care, to implement fast merging algorithms)\n",
    "# so we can just process them in parallel, and append them as they complete.\n",
    "bar = tqdm(total=len(doc_ids))\n",
    "tasks = [process_document(docid, doc_index) for doc_index, docid in enumerate(doc_ids)]\n",
    "await asyncio.gather(*tasks)\n",
    "bar.close()\n",
    "\n",
    "inverted_index = dict(inverted_index)  # Convert back to normal dict for pickling\n",
    "\n",
    "os.makedirs(\"../data/cache/\", exist_ok=True)\n",
    "\n",
    "# Save the inverted index to disk\n",
    "with open(\"../data/cache/inverted_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(inverted_index, f)\n",
    "\n",
    "# Save the document IDs to disk\n",
    "with open(\"../data/cache/doc_ids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(doc_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small pat on my own back: This process would've taken 45 minutes if it weren't for the async calls. With them, it took 26 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing quick search of query string 'machine learning' with over documents in index 'arxiv'\n",
      "Vectorizer loaded in 0.00 seconds\n",
      "Query vector computed in 0.04 seconds\n",
      "Inverted index loaded in 2.07 seconds\n",
      "Accumulated similarities for 7392 documents in 0.00 seconds\n",
      "Normalized similarities in 325.33 seconds\n",
      "Sorted top 10 results in 0.01 seconds\n",
      "Total query time: 327.44 seconds\n",
      "[('30749', 0.499339916162954),\n",
      " ('29106', 0.4550250906242585),\n",
      " ('25230', 0.45238527762709785),\n",
      " ('30639', 0.4255856152643423),\n",
      " ('18989', 0.3944888658727178),\n",
      " ('53877', 0.3944888658727178),\n",
      " ('37272', 0.3767314656563681),\n",
      " ('55842', 0.3767314656563681),\n",
      " ('26955', 0.35511371168748196),\n",
      " ('35642', 0.3518888496131795)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import math\n",
    "from elastic.vectorizer import TFIDFVectorizer\n",
    "\n",
    "\n",
    "print(f\"Executing quick search of query string '{query_string}' with over documents in index '{index_name}'\")\n",
    "start = time.time()\n",
    "vectorizer = TFIDFVectorizer(client, index_name=index_name, analyzer_name=analyzer_name)\n",
    "vectorizer_loaded_time = time.time()\n",
    "print(f\"Vectorizer loaded in {vectorizer_loaded_time-start:.2f} seconds\")\n",
    "\n",
    "# Compute the query vector. No need to normalize it, as we only care about relative similarities.\n",
    "query_vector = vectorizer.tf_idf_text(query_string)\n",
    "query_norm = math.sqrt(sum(weight**2 for weight in query_vector.values()))\n",
    "query_vector_normalized = {token: weight / query_norm for token, weight in query_vector.items()}\n",
    "\n",
    "\n",
    "query_vector_computed_time = time.time()\n",
    "print(f\"Query vector computed in {query_vector_computed_time-vectorizer_loaded_time:.2f} seconds\")\n",
    "\n",
    "# Fast implementation:\n",
    "# Scan through the query terms, look them up in the inverted index, and accumulate partial similarities\n",
    "\n",
    "# Start by loading the inverted index and document IDs from disk\n",
    "with open(\"../data/cache/inverted_index.pkl\", \"rb\") as f:\n",
    "    inverted_index = pickle.load(f)\n",
    "with open(\"../data/cache/doc_ids.pkl\", \"rb\") as f:\n",
    "    doc_ids = pickle.load(f)\n",
    "\n",
    "inverted_index_loaded_time = time.time()\n",
    "print(f\"Inverted index loaded in {inverted_index_loaded_time-query_vector_computed_time:.2f} seconds\")\n",
    "\n",
    "similarities = defaultdict(float)\n",
    "for term, weight in query_vector_normalized.items():\n",
    "    if term in inverted_index:\n",
    "        for doc_index, doc_weight in inverted_index[term]:\n",
    "            docid = doc_ids[doc_index]\n",
    "            similarities[docid] += weight * doc_weight\n",
    "\n",
    "similarities_aggregated_time = time.time()\n",
    "print(\n",
    "    f\"Accumulated similarities for {len(similarities)} documents in {similarities_aggregated_time-inverted_index_loaded_time:.2f} seconds\"\n",
    ")\n",
    "\n",
    "for docid in similarities:\n",
    "    doc_tf_idf = vectorizer.tf_idf_document(docid)\n",
    "    doc_tf_idf_norm = math.sqrt(sum(weight**2 for weight in doc_tf_idf.values()))\n",
    "    similarities[docid] /= doc_tf_idf_norm\n",
    "\n",
    "similarities_normalized_time = time.time()\n",
    "print(f\"Normalized similarities in {similarities_normalized_time-similarities_aggregated_time:.2f} seconds\")\n",
    "\n",
    "# now sort by cosine similarity\n",
    "sorted_answer = sorted(similarities.items(), key=lambda kv: kv[1], reverse=True)\n",
    "response_time = time.time()\n",
    "print(f\"Sorted top {r} results in {response_time-similarities_normalized_time:.2f} seconds\")\n",
    "print(f\"Total query time: {response_time-start:.2f} seconds\")\n",
    "pprint(sorted_answer[:r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, querying elasticsearch each time for the norm of the tf-idf vectors of the documents is very inefficient, so we'll try to also cache this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With caching of TF-IDF norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58102 [00:00<?, ?it/s]Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x00000106BBB1A900>\n",
      "100%|██████████| 58102/58102 [00:28<00:00, 2023.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from elastic.vectorizer import AsyncTFIDFVectorizer\n",
    "from elasticsearch import AsyncElasticsearch\n",
    "\n",
    "async_client = AsyncElasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "vectorizer = AsyncTFIDFVectorizer(async_client, index_name=index_name, analyzer_name=analyzer_name)\n",
    "vector_norms: List[float] = []  # Again, we'll store floats in a list for memory and access efficiency\n",
    "\n",
    "\n",
    "with open(\"../data/cache/doc_ids.pkl\", \"rb\") as f:\n",
    "    doc_ids: List[str] = pickle.load(f)\n",
    "\n",
    "bar = tqdm(total=len(doc_ids))\n",
    "\n",
    "\n",
    "async def compute_norm(docid: str) -> float:\n",
    "    doc_tf_idf = await vectorizer.tf_idf_document(docid)\n",
    "    doc_tf_idf_norm = math.sqrt(sum(weight**2 for weight in doc_tf_idf.values()))\n",
    "    bar.update(1)\n",
    "    return doc_tf_idf_norm\n",
    "\n",
    "\n",
    "async def main():\n",
    "    tasks = [compute_norm(docid) for docid in doc_ids]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "vector_norms = await main()\n",
    "bar.close()\n",
    "\n",
    "with open(\"../data/cache/vector_norms.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vector_norms, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing quick search of query string 'machine learning' with over documents in index 'arxiv'\n",
      "Vectorizer loaded in 0.00 seconds\n",
      "Query vector computed in 0.05 seconds\n",
      "Inverted index loaded in 1.27 seconds\n",
      "Accumulated similarities for 7392 documents in 0.00 seconds\n",
      "Normalized similarities in 0.00 seconds\n",
      "Sorted top 10 results in 0.00 seconds\n",
      "Total query time: 1.32 seconds\n",
      "(30749, 0.499339916162954)\n",
      "(29106, 0.4550250906242585)\n",
      "(25230, 0.45238527762709785)\n",
      "(30639, 0.4255856152643423)\n",
      "(18989, 0.3944888658727178)\n",
      "(53877, 0.3944888658727178)\n",
      "(37272, 0.3767314656563681)\n",
      "(55842, 0.3767314656563681)\n",
      "(26955, 0.35511371168748196)\n",
      "(35642, 0.3518888496131795)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "print(f\"Executing quick search of query string '{query_string}' with over documents in index '{index_name}'\")\n",
    "start = time.time()\n",
    "vectorizer = TFIDFVectorizer(client, index_name=index_name, analyzer_name=analyzer_name)\n",
    "vectorizer_loaded_time = time.time()\n",
    "print(f\"Vectorizer loaded in {vectorizer_loaded_time-start:.2f} seconds\")\n",
    "\n",
    "# Compute the query vector. No need to normalize it, as we only care about relative similarities.\n",
    "query_vector = vectorizer.tf_idf_text(query_string)\n",
    "query_norm = math.sqrt(sum(weight**2 for weight in query_vector.values()))\n",
    "query_vector_normalized = {token: weight / query_norm for token, weight in query_vector.items()}\n",
    "\n",
    "\n",
    "query_vector_computed_time = time.time()\n",
    "print(f\"Query vector computed in {query_vector_computed_time-vectorizer_loaded_time:.2f} seconds\")\n",
    "\n",
    "# Fast implementation:\n",
    "# Scan through the query terms, look them up in the inverted index, and accumulate partial similarities\n",
    "\n",
    "# Start by loading the inverted index and document IDs from disk\n",
    "with open(\"../data/cache/inverted_index.pkl\", \"rb\") as f:\n",
    "    inverted_index = pickle.load(f)\n",
    "with open(\"../data/cache/doc_ids.pkl\", \"rb\") as f:\n",
    "    doc_ids = pickle.load(f)\n",
    "with open(\"../data/cache/vector_norms.pkl\", \"rb\") as f:\n",
    "    vector_norms = pickle.load(f)\n",
    "\n",
    "inverted_index_loaded_time = time.time()\n",
    "print(f\"Inverted index loaded in {inverted_index_loaded_time-query_vector_computed_time:.2f} seconds\")\n",
    "\n",
    "similarities = defaultdict(float)\n",
    "for term, weight in query_vector_normalized.items():\n",
    "    if term in inverted_index:\n",
    "        for doc_index, doc_weight in inverted_index[term]:\n",
    "            similarities[doc_index] += weight * doc_weight\n",
    "\n",
    "similarities_aggregated_time = time.time()\n",
    "print(\n",
    "    f\"Accumulated similarities for {len(similarities)} documents in {similarities_aggregated_time-inverted_index_loaded_time:.2f} seconds\"\n",
    ")\n",
    "\n",
    "for doc_index in similarities:\n",
    "    similarities[doc_index] /= vector_norms[doc_index]\n",
    "\n",
    "similarities_normalized_time = time.time()\n",
    "print(f\"Normalized similarities in {similarities_normalized_time-similarities_aggregated_time:.2f} seconds\")\n",
    "\n",
    "# now sort by cosine similarity\n",
    "sorted_answer = sorted(similarities.items(), key=lambda kv: kv[1], reverse=True)\n",
    "response_time = time.time()\n",
    "print(f\"Sorted top {r} results in {response_time-similarities_normalized_time:.2f} seconds\")\n",
    "print(f\"Total query time: {response_time-start:.2f} seconds\")\n",
    "for doc_index, score in sorted_answer[:r]:\n",
    "    docid = doc_ids[doc_index]\n",
    "    print(f\"({docid}, {score})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila, what took 45 minutes the first time, was reduced to 1.02 seconds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly using Elasticsearch's built-in search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search finished in 0.0216 seconds\n",
      "Results:\n",
      "(30749, 9.954108)\n",
      "(25230, 9.918955)\n",
      "(29106, 9.918955)\n",
      "(18989, 9.913799)\n",
      "(53877, 9.913799)\n",
      "(32144, 9.854071)\n",
      "(37272, 9.836382)\n",
      "(55842, 9.836382)\n",
      "(30639, 9.817394)\n",
      "(35642, 9.597559)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "search_start = time.time()\n",
    "results = client.search(index=index_name, body={\"query\": {\"match\": {\"text\": query_string}}, \"size\": r})\n",
    "search_end = time.time()\n",
    "print(f\"Search finished in {search_end-search_start:.4f} seconds\")\n",
    "print(\"Results:\")\n",
    "for hit in results[\"hits\"][\"hits\"]:\n",
    "    print(f\"({hit['_id']}, {hit['_score']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab4 (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
